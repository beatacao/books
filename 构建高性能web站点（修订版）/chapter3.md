# 服务器并发处理能力  

## 3.1 吞吐率  
    . 吞吐率：web服务器单位时间内处理的请求数，（reqs/s）  
    . 最大吞吐率：吞吐率，仅表示了服务器在实际运行期间单位时间内处理的请求数。我们有时更关心的是最大吞吐率（即服务器在单位时间内所能处理的最大请求数），一般通过压力测试（模拟足够数量的并发请求，持续请求一段时间，最后求平均值）来得出  
    . 因为web服务器受理的请求的多样化，用压力测试模拟真实运行的并发非常困难。（在有多个变量的数据模型种求最优结果是很困难的，我们一般简化模型，对有代表性的特定类型请求，分别进行压力测试，求出最大吞吐率。最后根据需要，对各个吞吐率按照比例求加权平均值）  
    . 也正式因为web服务器需要受理请求的多样化，需要针对不同请求做不同的优化。（就像银行柜台，针对不同的业务有特殊的窗口。如果每个窗口都可以受理所有业务，整体效率势必会下降，因为每个银行柜台服务人员不可能了解银行的所有业务）  

### 压力测试前提条件  
    我们要进行压力测试，需要有一些前提条件：压力描述（并发用户数和总请求数）和请求性质描述（请求资源描述），即： 

    . 并发用户数  
    . 总请求数  
    . 请求资源描述  

### 并发用户数（别名：并发数，连接数）   

    . 单位时间内同时向服务器发送请求的用户数。（注意和吞吐率区分，吞吐率是单位时间内服务器可处理请求的一个量化表示）。  
    . 最大并发用户数：‘最大’是服务和用户之间经过权衡的一个平衡点。（银行柜台例子）  
    . 通过压力测试得到的最大并发用户数，并不能代表服务器真实并发用户数。得出最大并发用户数的意义，在于了解服务器的承载能力，并结合实际用户规模考虑适当的扩展方案，从商业角度来看，最大并发数比吞吐率更容易理解。  
    . 最大并发用户数和最大并发连接数的决定性因素是有区分的 -- 请求的性质：服务器处理每个请求需要的时间。  

### 请求等待时间  

    关注两个指标：  
    . 用户平均请求等待时间： 用于衡量在一定并发用户数量的情况下，服务器对于单个用户的服务质量    
    . 服务器平均请求处理时间：用户衡量服务器的整体服务质量  

### 进行压力测试 （例： apache ab，主要用户服务器端性能压测，还有一些运行在客户端的压测软件。）  

    本次压测，主要针对服务器请求处理能力的测试，不包含客户端：
    . 压力测试使用指标：请求总数，并发用户数，请求资源  
    . 过程：增加请求并发用户数，观察压测结果中服务器性能的各项指标，如： 吞吐率，服务器平均请求处理时间，用户平均请求等待时间，找到平衡点：最大并发用户数  
    . 在一定用户并发数下，影响服务器各种性能指标的因素，除去硬件外，就是服务器的并发策略了。  
    . 并发策略，即在服务器需要同时处理较多请求的时候，如何合理协调并充分利用cpu计算和I/O操作，使其在较大并发用户数的情况下有较大的吞吐率。  


## 3.2 cpu并发计算  
    
    服务器能够同时处理多个请求，原因在于操作系统的多执行流体系设计允许多个任务轮流使用系统资源，这些资源包括：cpu, 内存和 I/O等。  

### 进程  
    
    . 多执行流的一般实现即进程。多进程的好处不只在于对资源的轮流使用，也在于对cpu和I/O的叠加利用。（I/O包括磁盘I/O和网络I/O, I/O的速度和cpu速度相比，犹如老牛漫步和超音速飞机相距甚远）。  
    . 使用fork()系统调用创建进程。进程之间是互不影响的，可以通信，但不能干涉彼此的内存空间。因此，当大量使用进程时，会因内存占用太多而有性能问题。  
    . 使用clone()系统调用创建轻量级进程。这些进程之间可以共享一部分数据，较一般进程省去一部分内存控件。但当大量使用进程时，仍会因为进程之间需要频繁切换而影响性能。  
    . 进程的调度由内核直接来进行，从内核的观点看，进程的目的就是担当分配系统资源的实体。  

### 线程  

    分为用户态线程和内核级线程：  
    用户态线程：从内核的角度，多个用户态线程就是一个进程，这些线程由用户态使用一些库函数模拟实现的多执行流。因为不由内核直接分配资源，在多处理器服务器（SMP）上表现较差。  
    内核级线程：使用clone创建，可见内核级线程实现的原理，即把线程和轻量级进程进行一对一关联，每个线程实际上就是一个轻量级进程，由于由内核直接分配资源，对于SMP支持良好。但是在线程切换上，比用户态线程开销多一些。  

### 进程调度器（scheduler）  
    
    负责决定下一个执行的进程。维护着几个不同状态的进程队列。每个进程都有自己的优先级，进程调度器可以适当调整进程的优先级，目标是为了让所有进程更好的重叠利用系统资源。  

### 系统负载  

    系统资源繁忙程度；工具：uptime ,top , w  

### 进程切换  

    当进程数量达到一定程度时，进程之间的切换是一笔不容忽视的开销，因此高性能网站应该考虑尽量少进程，使用线程。    

## 3.3 系统调用  

    进程分为两种运行模式： 内核态和用户态。这两种模式之间可以相互转换。  
    进程一般处于用户态模式，这时可以使用cpu和内存完成一些任务。当需要对硬件外设进程操作时（如发送网络数据，读取磁盘等），需要转换到内核态模式，这种模式下，进程拥有更多的权利来操控计算机。当在内核态的任务完成后，进程又切换回用户态。  
    由用户态转换为内核态，调用系统接口即可。  
    频繁进行系统调用，会导致这两种模式的频繁切换，从而影响服务器的吞吐率，因此要降低请求处理时间，需要尽量减少系统调用。  

## 3.4 内存分配  

    web服务器在工作过程中，会使用大量的内存，因此内存的分配和释放尤其重要。  
    对于传统的服务器而言，各类表达式最大的开销在于中间临时变量的内存分配和数据复制时间。  
    而对于web服务器提供成千上万的http请求而言，内存堆栈的分配和复制次数大大增加，我们可以通过改善数据结构和算法复杂度来降低数据复制时间。  
    对于web服务器而言，各个服务器都采用了不同的内存分配策略。（举例：apache, ngiux, lightptd）  
    内存分配策略的设计是服务器的并发处理能力的重要保障。  

## 3.5 I/O 模型  

> 阻塞/非阻塞，同步/异步  

    “阻塞”与"非阻塞"与"同步"与“异步"不能简单的从字面理解，提供一个从分布式系统角度的回答。  
    1.同步与异步 - 消息通信机制     
        同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)  
        所谓同步，就是在发出一个*调用*时，在没有得到结果之前，该*调用*就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由*调用者*主动等待这个*调用*的结果。  
        而异步则是相反，*调用*在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在*调用*发出后，*被调用者*通过状态、通知来通知调用者，或通过回调函数处理这个调用。   
        典型的异步编程模型比如Node.js举个通俗的例子：  
        你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下"，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。  
    2. 阻塞与非阻塞 - 等待结果时，线程的状态  
        阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.  
        阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。  
        非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。   
        还是上面的例子，你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。
